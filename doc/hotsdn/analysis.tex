\section{Motivating Examples}

In this section, we charaterize the workloads of two widely deployed service in today's data centers, highlighting the features influencing the design decisions of $A^2Net$.

\subsection{Shuffle}

Hadoop \cite{Hadoop} is an open source implementation of MapReduce \cite{MapReduce}. It's widely deployed in many data centers handling many throughput preferred tasks, e.g. update/backup the online/offline database, refresh the web index, etc.. The workflow of MapReduce/Hadoop jobs is divided into two phases, Map, generating the key-value pairs by processing the job input via user-defined \emph{Map()} function, and Reduce, which takes the output of Map phase as input and yields the final output by executing the user-defined \emph{Reduce()}. Both of these two phases are completed by the distributed tasks, \emph{Mappers} and \emph{Reducers}. The workflow of Shuffle is shown in Figure \ref{fig:shuffle}.

The output of Mappers are transfered through networks to Reducers, which is referred as \emph{Shuffle} in Hadoop context.Only when the reducer has fetched all inputs it needs, it can begin to preprocess the input data and execute the user-defined \emph{Reduce()}. The key space of the Mappers' output is split into multiple subspaces. By default, the number of subspaces is equal to the number of \emph{Reducers}. For a certain key-value pair, which reducer it will be sent to is determined by another function \emph{Partitioner()}. The keys of the key-value pairs generated by Mappers are passed as the input parameter of \emph{Partitioner()} yielding destination of the key-value pair.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{pic/placeholder}\\
  \caption{}
  \label{fig:shuffle}
\end{figure}

\subsubsection{Load Skew in Shuffle}

We have an insight to the default implementation of \emph{Partitioner()} in Hadoop. The receiver of the pair is calculated by hashing the key of the pair:

\begin{verbatim}
public int getPartition(K2 key, V2 value,  
                      int numPartitions) {  
return (key.hashCode()&Integer.MAX_VALUE) 
	%numPartitions;  
}  
\end{verbatim}

The random hash function makes the Shuffle stage be vulnerable to load skew. As shown in Figure \ref{fig:shuffle_uneven_load}, a Mapper may have more data to be sent to certain Reducer than others, while fairness-striving TCP protocol allocates the same bandwidth to all flows sent by this Mapper. So the overloaded flow may last longer than others, unnecessarily prolonging the completion time of the task, and in turn, the task becomes the \emph{Stragglers} or \emph{Outliers} \cite{Mantri} \cite{LATE}. 

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{pic/placeholder}\\
  \caption{}
  \label{fig:shuffle_uneven_load}
\end{figure}

To elaborate the problem of uneven load in Shuffle, we present a simulation on a 2048-server FatTree topology with a modest oversubscription factor setup \cite{DatacenterDesign} (4 from ToR to aggregate switches and 2 from aggregate to core switches). We create the Shuffle-like workload by randomly selecting a set of nodes from the simulated data center, say n nodes in total, to execute Mapper, and 0.95 * n nodes for Reducers. Factor 0.95 is recommended by the Hadoop official wiki \cite{ReduceNum}. The number of receivers for each Mappers and the partitions of data sent to Reducers are randomly selected for keeping consistent with the random hash based paritioner implementation. More details about simulation method are discussed in \ref{sec:eval}. To factor out the incluence brought by transport layer, we assume the router buffer is infinite and rate limiting is disabled. 

In Figure \ref{fig:shuffle_long_tail}, it shows the flow completion time in a 93-Mappers-88-Reducers MapReduce job. 

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{pic/placeholder}\\
  \caption{}
  \label{fig:shuffle_long_tail}
\end{figure}

\subsection{Partition-Aggregator}

Partition-Aggregator \cite{DCTCP} workloads are usually generated by the data center applications handling user requests, e.g. emails, search engine and social networks content composition, which are usually built with soft real-time style for interactive. As shown in Figure \ref{fig:par-agg}, requests from users are split into multiple \emph{sub-requests} by an aggregator in the root layer and delivered to the distributed worker processes in the system via aggregators on different layers. 

To meet the all-up SLA \cite{DCTCP}, aggregators usually assign deadlines to the worker nodes in lower layers. When deadline arrives, the aggregator will directly return the collected results back to the upper layer instead of waiting for all nodes to complete. Those discarded results in lagged behind nodes can ultimately lower the quality of the responses. As we explained in Introduction section, network transfer occupies a considerable part of total workloads of data center applications. The worker deadlines mean that network flows carring requests and responses have deadlines. Only the network flows finished within deadlines can contribute to the throughput of application, on another side, the slow and ultimately discarded flows can waste network bandwidth.

\subsubsection{Causes of Missing Deadlines}

However, the commonly used TCP protocol strives for maximizing network throughput while keeping fairness, being unaware of the deadlines of flows. The lack of prioritization among flows causes that \emph{urgent} flows wait behind latency-insensitive flows just for fairness, making them miss the deadlines. To examine this issue, we reproduce the workload consisting of both throughput-preferred but latency-insensitive flows and latency flows....


\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{pic/placeholder}\\
  \caption{}
  \label{fig:par-agg}
\end{figure}

